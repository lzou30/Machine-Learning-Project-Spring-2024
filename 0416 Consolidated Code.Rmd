---
title: "ML Project Group 1"
author: "Morgan Kleidon"
date: "2024-03-30"
output: pdf_document
---
  
```{r setup, include = FALSE}
# Clear all 
rm(list = ls())

# Setup
knitr::opts_chunk$set(echo = TRUE)

# Load packages
pacman::p_load(tidyverse, ggplot2, scales, glmnet, pls, randomForest, rpart, DescTools, dplyr, readxl, readr)

# Set custom theme
MK_theme <-   theme(text = element_text(family = "Times New Roman"),
                    panel.background = element_blank(),
                    axis.text.x.bottom = element_text(size = 12),
                    axis.text.y.left = element_text(size = 12),
                    axis.title.x.bottom = element_text(size = 16),
                    axis.title.y.left = element_text(size = 16),
                    title = element_text(size = 16, face = "bold"),
                    panel.border = element_rect(color = "black", linewidth = 0.3, fill = "transparent"),
                    panel.grid = element_line(color = "black", linewidth = 0.1),
                    legend.position = "none")
```

# 1. Go to Blackboard and download the files. Starting with the CFPB complaint data, please load the data into R and clean the data set. Please note, each team has a different random sample. 
## a. I am providing the code for the dependent variable “relief” from the company response variable. Please note, you will need to drop the company response variable before you do the modeling as this will perfectly predict the DV. 
## b. Your group needs to make decisions on how to treat the rest of this variable. In your write up, defend your answer. 
## i. How should “In progress” be coded? 
## ii.	Should the other conditions be coded into dummy variables? 
## c. All of the CFPB data documentation is located at https://cfpb.github.io/api/ccdb/fields.html . 
## i. While not required, it would be in your best interest to recode some of these variables to have shorter values in the name. 

```{r, echo = TRUE}
# Bring in data 
group1 <- read_csv("C:/Users/lzou7/OneDrive/Desktop/GWU/ML in R/Project/group1.csv")
colnames(group1)
group1 <- group1[,-1]

# Add leading zero
add_char <- function(x, pos, insert) {      
  gsub(paste0("^(.{", pos, "})(.*)$"),
       paste0("\\1", insert, "\\2"),
       x)
}



which(nchar(group1$ZIP.code) == 4) # no four-digit zip codes
group1.1 <- group1 %>%
  filter(State != "AE" & State != "AP") # Remove non-state observations

# Load in zip_fips
zip_fips <- read.csv("C:/Users/lzou7/OneDrive/Desktop/GWU/ML in R/Project/zip_fips.csv", colClasses = c(ZIP = "character"))
colnames(zip_fips)
zip_fips <- zip_fips[,-1]
str(zip_fips$ZIP)
table(zip_fips$STATE)

# Filter non-US states
zip_fips_states <- zip_fips %>%
  filter(STATE != "PR" & STATE != "GU" & STATE != "DC" & STATE != "VI")

# Add a leading zero if zip is only four characters long
zip_fips_states$ZIP <- ifelse(nchar(zip_fips_states$ZIP) == 4, add_char(x = zip_fips_states$ZIP,
                                                                        pos = 0, 
                                                                        insert = "0"), zip_fips_states$ZIP)

# Merge datasets to find odd zip codes
group1.2 <- merge(group1.1, zip_fips_states, 
                  by.x = c("ZIP.code", "State"),
                  by.y = c("ZIP", "STATE"), all.x = TRUE)

group1.2$ZIP.code <- sprintf("%05d", group1.2$ZIP.code)
group1.2$missing_zip <- as.factor(is.na(group1.2$STCOUNTYFP))
group1.2 <- group1.2[,-15] # Drop company response to consumer

# group1_merged$missing <- as.factor(complete.cases(group1_merged))
# group1_merged <- group1_merged[,-c(20:21)]
# group1_merged$ZIP.code <- as.factor(group1_merged$ZIP.code)
# noy <- is.na(group1_merged$relief)
# group1_merged <- group1_merged[which(noy == 0),]


```

# 2. Your first task is to clean the zip code variable. Many of the zip codes are missing, incomplete, or missing the leading zero. Your group must pick a missing data method to fix this data!
## a.	For the dependent variable (relief) and 3 other variables of your choosing, do a simple statistical comparison (2 sample proportions or t-test, Chi-Square,…) to check for differences between the subgroups that are missing zip code and those that provided the information. 
## i. Suggestion: You will want to create a dummy variable to indicate if the variable is missing for the rest of the analysis. Using the aggregate() function in r, this should be an easy task. 


```{r}
# Submitted.via
table(group1.2$Submitted.via, group1.2$missing_zip)
group1.3 <- group1.2 %>%
  mutate(Submitted.via = case_when(Submitted.via == "Web Referral" ~ "Referral",
                                   Submitted.via == "Web" ~ "Web", 
                                   Submitted.via == "Referral" ~ "Referral",
                                   Submitted.via == "Postal mail" ~ "Postal mail",
                                   Submitted.via == "Phone" ~ "Phone",
                                   Submitted.via == "Fax" ~ "Fax")) # Recode Web Referral to Referral
table(group1.3$Submitted.via, group1.3$missing_zip) # Meets assumptions now :) 
chisq.test(group1.3$Submitted.via, group1.3$missing_zip) # p-value significant

# Tags
table(group1.3$Tags, group1.3$missing_zip)
fisher.test(group1.3$Tags, group1.3$missing_zip, workspace = 2e+09) # Does not meet assumptions for Chi-Square

# Relief
table(group1.3$relief, group1.3$missing_zip)
chisq.test(group1.3$relief, group1.3$missing_zip) # p-value = 1

# Timely.response
table(group1.3$Timely.response., group1.3$missing_zip)
chisq.test(group1.3$Timely.response., group1.3$missing_zip) # p-value *almost* significant

```

## b. Please note, you have the state in all cases. You must fix the fill this field with a zip code that is a plausible value (the corrected/imputed zip code must be from the State listed). 
## i. You can drop any non-state in the analysis. (“DC”, “UNITED STATES MINOR OUTLYING ISLANDS”, “AA” (Armed Forces), …) 
## c. Any imputation method is acceptable as long as you defend your choice. You can use mean/mode replacement, nearest neighbor, regression based models, …
```{r}
# Use mode to impute zip code
group1.4 <- group1.3 %>%
  group_by(State) %>%
  mutate(imp_zip = ifelse(missing_zip == TRUE, Mode(ZIP.code),  ZIP.code))

# Could use rfImpute to impute the incorrect zip codes, but would need to convert the wrong ones into NAs

```

## d.	Please indicate any limitations or biases your imputation has on the analysis? 
## i. If I replace all values with the state capital zip code, how would that impact my analysis?
\textcolor{red}{Indicate any limitations here.}

# 3.	Merge on the FIPS county code for each Zip code I have provided in the folder. 
## a.	All zip codes should have a FIPS county code. If it does not, 
## i.	Check to make sure you formatted the data right (i.e. 2019 will not merge with 02019 correctly if both are characters.)
## ii.	If the zip code is nonexistent (someone made it up, like 99999), then you need to treat it as missing and replace it using the same method as in part 2. 
## iii.	*** Zip codes to change, and it is possible that the Zip to Fips code file I am using from last year misses a new observation. Just treat it as missing and move on. 
```{r}
group1.5 <- group1.4[,-c(19:20)] # remove COUNTYNAME & STCOUNTYFP
group1.6 <- merge(group1.5, zip_fips_states, by.x = c("imp_zip", "State"), by.y = c("ZIP", "STATE")) # merge

# Add a leading zero to the group1.6 FIPS code
group1.6$fips <- sprintf("%05d", group1.6$STCOUNTYFP)

```

# 4. Merge on county level debt information from The Urban Institute for 2022 on by county FIPS code.
## a.	Suggestion: Be careful how you load the data to make sure missing values do not change everything to text. 
## b. There is other debt data at the site above if you want to add it also, but it is not required. 
## c. Remember to do left joins/merges on the data. The data file also has state totals you want to drop from the analysis. 
```{r}
# Bring in data from the Urban Institute
county_debt <- read_excel("C:/Users/lzou7/OneDrive/Desktop/GWU/ML in R/Project/dia_lbls_all_overall_county_2022_02_14Sep2023.xlsx")

# Change column names
names(county_debt)[names(county_debt) == "County FIPS"] <- "fips"

# Merge county debt data with group1.6
group1.7 <- merge(group1.6, county_debt, by = "fips", all.x = TRUE)

```

# 5. Load the Census demographic file into R. Before you merge it into the debt collection data, you need to combine the State and County FIPs to merge the data and then develop features. Each group need to add measures that would indicate bias in debt collection against: 
## a. Gender
## b. Age
## i. Bias against young (0-24 years) and old (65+) are of particular concern
## ii. You need to create a dummy variable for county level older Americans. 
## c. Race
## d. Ethnicity 
## i. Hispanic vs. non-Hispanic at minimum. 
## e. Also, from the original CFPB dataset create dummy variables for Service Members and Older American based on the Tags column. 

```{r}
# Read in Census data
census <- read_csv("C:/Users/lzou7/OneDrive/Desktop/GWU/ML in R/Project/cc-est2022-all.csv")
colnames(census)

# Add leading zeros to county and state FIPS
census$STATE2 <- sprintf("%02s", census$STATE)
census$COUNTY2 <- sprintf("%03s", census$COUNTY)

# Append state to county FIPS
census$STCOUNTY <- paste0(census$STATE2, census$COUNTY2)

# Move STCOUNTY to the first column of the dataset
census <- census %>%
  relocate(STCOUNTY, .before = SUMLEV)

# Subsetting for most recent year and for total
census1 <- subset(census, YEAR == 4)
census2 <- subset(census1, AGEGRP == 0)

# Subsetting for youth
censusyoung <- census1 %>%
  filter(AGEGRP %in% c('1', '2', '3', '4', '5'))

# Subsetting for olds
censusold <- census1 %>%
  filter(AGEGRP %in% c('14', '15', '16', '17', '18'))

censusyoungsum <- censusyoung %>%
  group_by(STCOUNTY) %>%
  summarize(
    YOUTH_MALE = sum(TOT_MALE),
    YOUTH_FEMALE = sum(TOT_FEMALE),
  )

censusoldsum <- censusold %>%
  group_by(STCOUNTY) %>%
  summarize(
    OLD_MALE = sum(TOT_MALE),
    OLD_FEMALE = sum(TOT_FEMALE),
  )

census3 <- merge(census2, censusyoungsum, by = "STCOUNTY")
census3 <- merge(census3, censusoldsum, by = "STCOUNTY")

census3$pct_old <- (census3$OLD_FEMALE + census3$OLD_MALE)/census3$TOT_POP
census3$pct_young <- (census3$YOUTH_FEMALE + census3$YOUTH_MALE)/census3$TOT_POP
census3$pct_blk <- (census3$BA_FEMALE + census3$BA_MALE)/census3$TOT_POP
census3$pct_asian <- (census3$AA_FEMALE + census3$AA_MALE)/census3$TOT_POP
census3$pct_native <- (census3$IA_FEMALE + census3$IA_MALE)/census3$TOT_POP
census3$pct_his <- (census3$H_FEMALE + census3$H_MALE)/census3$TOT_POP
census3$pct_old <- (census3$OLD_FEMALE + census3$OLD_MALE)/census3$TOT_POP
census3$pct_old <- (census3$OLD_FEMALE + census3$OLD_MALE)/census3$TOT_POP
census3$pct_women <- (census3$TOT_FEMALE)/census3$TOT_POP

census3$COUNTY_OLD <- ifelse(census3$pct_old >= 0.5, 1, 0)
# old county = Sumnter, Florida

# Merge debt data with Census data
group1.8 <- merge(group1.7, census3, by.x = "fips", by.y = "STCOUNTY")

```

e and 6-ish
```{r}
# Create dummies for old or service member
group1.8$ServMember <- ifelse(grepl("Servicemember", group1.8$Tags), 1, 0)
group1.8$Old <- ifelse(grepl("Older American", group1.8$Tags), 1, 0)
```

```{r}
# Normalizing the Data
#First make a clean debt data dataframe
#note: 
#make a dataframe with numeric variables of interest
debt_clean <- as.data.frame(county_debt[, -c(1,2,3, 25:28)])
debt_clean <- debt_clean %>%
  mutate(across(everything(), as.numeric))
#debt_clean <- lapply(debt_clean, as.numeric)
#debt_clean <- debt_clean[, -c(1)]

#clean up variable names
colnames(debt_clean) <- abbreviate(colnames(debt_clean), minlength = 4)

debt_clean1 <- debt_clean

debt_clean1$NAs <- apply(debt_clean1, 1, function(row) any(is.na(row)))

impute_with_mode <- function(series) {
  mode_value <- as.character(sort(table(series), decreasing = TRUE)[1])  # Calculate the mode
  return(ifelse(is.na(series), mode_value, series))}

for (col in names(debt_clean1)) {
  debt_clean1[[col]] <- impute_with_mode(debt_clean1[[col]])
}

debt_clean1$imp <- ifelse(debt_clean1$NAs, "Imputed", "Not Imputed")
debt_clean2 <- debt_clean1[,-c(22:23)]
debt_clean2 <- debt_clean2 %>%
  mutate(across(everything(), as.numeric))

library(corrplot)

corr_matrix <- cor(debt_clean2, use = "pairwise.complete.obs")

library(ggcorrplot)
ggcorrplot(corr_matrix) #maybe try three different groups: all Comm of color, and white comm
#codebook pending
```
# 7
```{r}
# Standardize values
#clean_debt2 <- clean_debt[,-c()]
debt <- debt_clean2
debt[] <- lapply(debt, as.numeric)

library("factoextra")
debt.pca <- princomp(corr_matrix)
summary(debt.pca)
fviz_eig(debt.pca, addlabels = TRUE)

debt_pca <- prcomp(debt, scale = TRUE) # automatically standardizes the data

# PCA loadings
debt.pca$loadings[, 1:5]

loadings_matrix <- debt_pca$rotation
pcdebtscores <- debt_pca$x

county_debt1 <- county_debt[, -c(4:28)]
county_debt1 <- cbind(county_debt1, pcdebtscores[, 1:2])

group1.9 <- group1.8[,-c(26:46)]
group1.9 <- group1.9[,-c(30:34)]
group1.9 <- merge(group1.9, county_debt1, by = "fips", all.x = TRUE)
group1.9 <- group1.9[,-c(121:122)]
group1.9 <- group1.9[,-c(105:106)]
```
# 8

```{r}
library(clustMixType)
library("wesanderson")

group1.9$medical_debt_binary <- ifelse(group1.9$Sub.product == "Medical debt", 1, 0)
colnames(group1.9)

z <- group1.9[,c(121,26,27,117,118)]
z[,1]<- as.factor(z[,1] )
z[,2]<- scale(as.numeric(z[,2] ))
z[,3]<- scale(as.numeric(z[,3] ))
z[,4]<- as.factor(z[,4] )
z[,5]<- as.factor(z[,5] )

# 2 cluster
kpres2 <-kproto(x=z,k=2)
# 3 cluster
kpres3 <-kproto(x=z,k=3)

# Plots
clprofiles(kpres2, z, col=wes_palette("Royal1",4, type="continuous")) 

n.scree <- ncol(z) - 2

Es <- numeric(n.scree)

for (i in 1:n.scree) {
    kpres <- kproto(z, k = i, nstart = 5, verbose = FALSE)
    Es[i] <- kpres$tot.withinss
}

plot(1:n.scree, Es[1:3], type = "b", ylab = "Objective Function",
     xlab = "# Clusters", main = "Scree Plot")


```
