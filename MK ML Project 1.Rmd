---
title: "ML Project Group 1"
author: "Morgan Kleidon"
date: "2024-03-30"
output: html_document
---

```{r setup, include = FALSE}
# Clear all 
rm(list = ls())

# Setup
knitr::opts_chunk$set(echo = TRUE)

# Load packages
pacman::p_load(tidyverse, ggplot2, scales, glmnet, pls, randomForest, rpart, DescTools, readxl, corrplot, ggcorrplot, factoextra, ggfortify, clustMixType, wesanderson, caret, rpart.plot)

# Set custom theme
MK_theme <-   theme(text = element_text(family = "Times New Roman"),
                    panel.background = element_blank(),
                    axis.text.x.bottom = element_text(size = 12),
                    axis.text.y.left = element_text(size = 12),
                    axis.title.x.bottom = element_text(size = 16),
                    axis.title.y.left = element_text(size = 16),
                    title = element_text(size = 16, face = "bold"),
                    panel.border = element_rect(color = "black", linewidth = 0.3, fill = "transparent"),
                    panel.grid = element_line(color = "black", linewidth = 0.1),
                    legend.position = "none")
```

# 1. Go to Blackboard and download the files. Starting with the CFPB complaint data, please load the data into R and clean the data set. Please note, each team has a different random sample. 
## a. I am providing the code for the dependent variable “relief” from the company response variable. Please note, you will need to drop the company response variable before you do the modeling as this will perfectly predict the DV. 
## b. Your group needs to make decisions on how to treat the rest of this variable. In your write up, defend your answer. 
## i. How should “In progress” be coded? 
## ii.	Should the other conditions be coded into dummy variables? 
## c. All of the CFPB data documentation is located at https://cfpb.github.io/api/ccdb/fields.html . 
## i. While not required, it would be in your best interest to recode some of these variables to have shorter values in the name. 

```{r, echo = TRUE}
# Bring in data 
group1 <- read_csv("/Users/morgankleidon/Desktop/Spring 2024/Machine Learning/Data/group1.csv")
colnames(group1)
group1 <- group1[,-1]

# Add leading zero
add_char <- function(x, pos, insert) {      
  gsub(paste0("^(.{", pos, "})(.*)$"),
       paste0("\\1", insert, "\\2"),
       x)
}

which(nchar(group1$ZIP.code) == 4) # no four-digit zip codes
group1.1 <- group1 %>%
  filter(State != "AE" & State != "AP") # Remove non-state observations

# Load in zip_fips
zip_fips <- read_csv("/Users/morgankleidon/Desktop/Spring 2024/Machine Learning/Data/zip_fips.csv")
colnames(zip_fips)
zip_fips <- zip_fips[,-1]
str(zip_fips$ZIP)
table(zip_fips$STATE)

# Filter non-US states
zip_fips_states <- zip_fips %>%
  filter(STATE != "PR" & STATE != "GU" & STATE != "DC" & STATE != "VI")

# Add a leading zero if zip is only four characters long
zip_fips_states$ZIP <- ifelse(nchar(zip_fips_states$ZIP) == 4, add_char(x = zip_fips_states$ZIP,
                                                                     pos = 0, 
                                                                     insert = "0"), zip_fips_states$ZIP)

# Merge datasets to find odd zip codes
group1.2 <- merge(group1.1, zip_fips_states, 
                       by.x = c("ZIP.code", "State"),
                       by.y = c("ZIP", "STATE"), all.x = TRUE)

group1.2$missing_zip <- as.factor(is.na(group1.2$STCOUNTYFP))
group1.2 <- group1.2[,-15] # Drop company response to consumer b/c it is perfectly correlated with relief

```

# 2. Your first task is to clean the zip code variable. Many of the zip codes are missing, incomplete, or missing the leading zero. Your group must pick a missing data method to fix this data!
## a.	For the dependent variable (relief) and 3 other variables of your choosing, do a simple statistical comparison (2 sample proportions or t-test, Chi-Square,…) to check for differences between the subgroups that are missing zip code and those that provided the information. 
## i. Suggestion: You will want to create a dummy variable to indicate if the variable is missing for the rest of the analysis. Using the aggregate() function in r, this should be an easy task. 


```{r}
# Submitted.via
table(group1.2$Submitted.via, group1.2$missing_zip)
group1.3 <- group1.2 %>%
  mutate(Submitted.via = case_when(Submitted.via == "Web Referral" ~ "Referral",
            Submitted.via == "Web" ~ "Web", 
            Submitted.via == "Referral" ~ "Referral",
            Submitted.via == "Postal mail" ~ "Postal mail",
            Submitted.via == "Phone" ~ "Phone",
            Submitted.via == "Fax" ~ "Fax")) # Recode Web Referral to Referral
table(group1.3$Submitted.via, group1.3$missing_zip) # Meets assumptions now :) 
chisq.test(group1.3$Submitted.via, group1.3$missing_zip) # p-value significant

# Tags
table(group1.3$Tags, group1.3$missing_zip)
fisher.test(group1.3$Tags, group1.3$missing_zip) # Does not meet assumptions

# Relief
table(group1.3$relief, group1.3$missing_zip)
chisq.test(group1.3$relief, group1.3$missing_zip) # p-value = 1

# Timely.response
table(group1.3$Timely.response., group1.3$missing_zip)
chisq.test(group1.3$Timely.response., group1.3$missing_zip) # p-value *almost* significant

```

## b. Please note, you have the state in all cases. You must fix the fill this field with a zip code that is a plausible value (the corrected/imputed zip code must be from the State listed). 
## i. You can drop any non-state in the analysis. (“DC”, “UNITED STATES MINOR OUTLYING ISLANDS”, “AA” (Armed Forces), …) 
## c. Any imputation method is acceptable as long as you defend your choice. You can use mean/mode replacement, nearest neighbor, regression based models, …
```{r}
# Use mode to impute zip code
group1.4 <- group1.3 %>%
  group_by(State) %>%
  mutate(imp_zip = ifelse(missing_zip == TRUE, Mode(ZIP.code),  ZIP.code))

```

## d.	Please indicate any limitations or biases your imputation has on the analysis? 
## i. If I replace all values with the state capital zip code, how would that impact my analysis?
\textcolor{red}{Indicate any limitations here.}

# 3.	Merge on the FIPS county code for each Zip code I have provided in the folder. 
## a.	All zip codes should have a FIPS county code. If it does not, 
## i.	Check to make sure you formatted the data right (i.e. 2019 will not merge with 02019 correctly if both are characters.)
## ii.	If the zip code is nonexistent (someone made it up, like 99999), then you need to treat it as missing and replace it using the same method as in part 2. 
## iii.	*** Zip codes to change, and it is possible that the Zip to Fips code file I am using from last year misses a new observation. Just treat it as missing and move on. 
```{r}
group1.5 <- group1.4[,-c(19:20)] # remove COUNTYNAME & STCOUNTYFP
group1.6 <- merge(group1.5, zip_fips_states, by.x = c("imp_zip", "State"), by.y = c("ZIP", "STATE")) # merge
```

# 4. Merge on county level debt information from The Urban Institute for 2022 on by county FIPS code.
## a.	Suggestion: Be careful how you load the data to make sure missing values do not change everything to text. 
## b. There is other debt data at the site above if you want to add it also, but it is not required. 
## c. Remember to do left joins/merges on the data. The data file also has state totals you want to drop from the analysis. 
```{r}
# Bring in data from the Urban Institute
county_debt <- read_excel("/Users/morgankleidon/Desktop/Spring 2024/Machine Learning/Data/dia_lbls_all_overall_county_2022_02_14Sep2023.xlsx")

# Change column names
names(county_debt)[names(county_debt) == "County FIPS"] <- "fips"

# Add a leading zero to the group1.6 FIPS code
group1.6$fips <- ifelse(nchar(group1.6$STCOUNTYFP) == 4, add_char(x = group1.6$STCOUNTYFP,
                                                                     pos = 0, 
                                                                     insert = "0"), group1.6$STCOUNTYFP)

# Merge county debt data with group1.6
group1.7 <- merge(group1.6, county_debt, by = "fips", all.x = TRUE)

```

# 5. Load the Census demographic file into R. Before you merge it into the debt collection data, you need to combine the State and County FIPs to merge the data and then develop features. Each group need to add measures that would indicate bias in debt collection against: 
## a. Gender
## b. Age
## i. Bias against young (0-24 years) and old (65+) are of particular concern
### ii. You need to create a dummy variable for county level older Americans. 
## c. Race
## d. Ethnicity 
### i. Hispanic vs. non-Hispanic at minimum. 
## e. Also, from the original CFPB dataset create dummy variables for Service Members and Older American based on the Tags column. 

```{r}
# Read in Census data
census <- read_csv("/Users/morgankleidon/Desktop/Spring 2024/Machine Learning/Data/cc-est2022-all.csv")
colnames(census)

# Add leading zeros to county and state FIPS
census$STATE2 <- sprintf("%02d", census$STATE)
census$COUNTY2 <- sprintf("%03d", census$COUNTY)

# Append state to county FIPS
census$STCOUNTY <- paste0(census$STATE2, census$COUNTY2)

# Move STCOUNTY to the first column of the dataset
census <- census %>%
  relocate(STCOUNTY, .before = SUMLEV)

# Subsetting for the total column (where AGEGRP = 0)
census1 <- subset(census, AGEGRP == 0)

# Find the mean grouped by STCOUNTY, AGEGRP because there are four years we need to collapse
census2 <- census %>%
  group_by(STCOUNTY, AGEGRP) %>%
  summarize(across(TOT_POP:HNAC_FEMALE, ~mean(., na.rm = TRUE)))

# Rename columns
colnames(census2) <- paste0(colnames(census2), "_MEAN")
names(census2)[names(census2) == "STCOUNTY_MEAN"] <- "STCOUNTY"
names(census2)[names(census2) == "AGEGRP_MEAN"] <- "AGEGRP"

# Subset to total 
census_total <- census2 %>%
  filter(AGEGRP == 0)

# Subsetting for youth (0-24)
censusyoung <- census2 %>%
  filter(AGEGRP %in% c(1, 2, 3, 4, 5))

# Subsetting for elderly (65+)
censusold <- census2 %>%
  filter(AGEGRP %in% c(14, 15, 16, 17, 18))

# Find the sum of young people by county
censusyoungsum <- censusyoung %>%
  group_by(STCOUNTY) %>%
  summarize(
    YOUTH_MALE = sum(TOT_MALE_MEAN),
    YOUTH_FEMALE = sum(TOT_FEMALE_MEAN),
  )

# Find the sum of elderly people by county
censusoldsum <- censusold %>%
  group_by(STCOUNTY) %>%
  summarize(
    OLD_MALE = sum(TOT_MALE_MEAN),
    OLD_FEMALE = sum(TOT_FEMALE_MEAN),
  )

# Merging the young/old variables with the rest of the census data
census3 <- merge(census_total, censusyoungsum, by = "STCOUNTY")
census4 <- merge(census3, censusoldsum, by = "STCOUNTY")

# Creating variables to indicate bias
census4$pct_old <- (census4$OLD_FEMALE + census4$OLD_MALE)/census4$TOT_POP_MEAN
census4$pct_young <- (census4$YOUTH_FEMALE + census4$YOUTH_MALE)/census4$TOT_POP_MEAN
census4$pct_blk <- (census4$BA_FEMALE_MEAN + census4$BA_MALE_MEAN)/census4$TOT_POP_MEAN
census4$pct_asian <- (census4$AA_FEMALE_MEAN + census4$AA_MALE_MEAN)/census4$TOT_POP_MEAN
census4$pct_native <- (census4$IA_FEMALE_MEAN + census4$IA_MALE_MEAN)/census4$TOT_POP_MEAN
census4$pct_hisp <- (census4$H_FEMALE_MEAN + census4$H_MALE_MEAN)/census4$TOT_POP_MEAN
census4$pct_women <- census4$TOT_FEMALE_MEAN/census4$TOT_POP_MEAN

# Share of elderly population in America in 2020 was 16.8%, if a county has over this threshold, it is considered to be an older county
census4$COUNTY_OLD <- ifelse(census4$pct_old > 0.168, 1, 0)

```

# 6. Once you have taken all the Census data and created a data frame of demographic measures with one row for each county, merge it onto your debt collection data frame. 
```{r}
# Merge debt data with Census data
group1.8 <- merge(group1.7, census4, by.x = "fips", by.y = "STCOUNTY")

# Create a flag for servicemembers and older Americans
group1.8$service_mem <- ifelse(grepl("Servicemember", group1.8$Tags), 1, 0)
group1.8$old <- ifelse(grepl("Older American", group1.8$Tags), 1, 0)
group1.8$old_service_mem <- ifelse(grepl("Older American, Servicemember", group1.8$Tags), 1, 0)
group1.8$none <- ifelse(grepl("None", group1.8$Tags), 1, 0)

```

# 7. The debt collection variables are highly correlated. Take all or a subset of at least 5 of them and try to create a principal component out of them. Merge the resulting principal component back onto the main data set. 
## a. You may choose to keep only a subset in the final PCA analysis, but you must start with at least 5 variables. 
### i. Often we start with a larger number but drop the ones that don’t fit. 
## b. You are welcome to keep more than one PC.
### i. Maybe 3 variables load on one and 4 on another PC. Then just load them as two new variables. 
## c. You may also try PCA on other variables in the data set to see if you get a better model as part of your feature selection. 
## d. Suggestion: Examine the percent missing for each variable as you are selecting variables. 

```{r}
# Create a subset of debt collection
debt_subset <- county_debt[, -c(1,2,3)]

# Abbreviate the names of the variables
colnames(debt_subset) <- abbreviate(colnames(debt_subset), minlength = 4)

# Make all variables numeric
debt_subset2 <- debt_subset %>%
  sapply(as.numeric) %>%
  as.data.frame()

# Get percentage of missing values per variable
num_missing <- colSums(is.na(debt_subset2))
pct_missing <- as.data.frame(num_missing/nrow(debt_subset2))
colnames(pct_missing) <- "pct_missing"
pct_missing <- rownames_to_column(pct_missing, var = "var_name")

# Create a variable to indicate missing values
debt_subset2$is_complete <- as.factor(complete.cases(debt_subset2))

# Can't have a correlation matrix with NAs, must impute
debt_subset2.imp <- na.roughfix(debt_subset2)

# Get the correlation matrix
debt_matrix <- cor(debt_subset2.imp[,-c(26)])
ggcorrplot(debt_matrix)

# Perform PCA on 5 MOST correlated variables (SwmdicWc, SwmdicA, SwadicA, AhiWc, AhiCoc, AhiA)
top_corr_matrix <- cor(debt_subset2.imp[,c(1, 3, 7, 9, 23:25)])
top_corr_sub <- debt_subset2.imp[,c(1, 3, 7, 9, 23:25)]

# Perform PCA
debt_pca <- prcomp(top_corr_sub, scale = TRUE)
summary(debt_pca)
fviz_eig(debt_pca, addlabels = TRUE)
loadings <- debt_pca$rotation
scores <- debt_pca$x

# PERFORM PCA ON ALL DEBT VARIABLES
# debt_pca <- prcomp(debt_subset2.imp[,-c(26)], scale = TRUE)
# summary(debt_pca)
# fviz_eig(debt_pca, addlabels = TRUE)
# loadings <- debt_pca$rotation
# scores <- debt_pca$x

# Merge back on to imputed county debt data
county_debt1 <- cbind(county_debt[,1:3], debt_subset2.imp)
county_debt1 <- county_debt1[, -c(4, 6, 10, 12, 26:28)]
county_debt1 <- cbind(county_debt1, scores[, 1:2])

# Merge back with the rest of the data
group1.9 <- group1.8[,-c(24:50)]
group1.9 <- merge(group1.9, county_debt1, by = "fips", all.x = TRUE)

```

# 8. Do a mixed-type Cluster analysis using the R package clustMixType. Create a clustering based on medical debt. 
## a.	Make a binary variable out of Sub Product == Medical Debt. 
## b. Pick 5 other variables that you believe may be related to medical debt. There must be at least 2 factors and 2 continuous. 
## c. Save the clusters as output and add them back to the main data set. 
```{r}
# Make binary variable out of Sub product == Medical Debt
group1.9$medical_debt <- ifelse((group1.9$Sub.product == "Medical debt" | group1.9$Sub.product == "Medical"), 1, 0)

z <- group1.9[, c("medical_debt", "old_service_mem",
                "pct_old", "pct_young",
                "SoslhwsldidA", "Consumer.disputed.")]

z[,1]<- as.factor(z[,1])
z[,2]<- as.factor(z[,2])
z[,3]<- scale(as.numeric(z[,3]))
z[,4] <- scale(as.numeric(z[,4]))
z[,5]<- scale(as.numeric(z[,5]))
z[,6] <- as.factor(z[,6])

# 2 cluster
kpres2 <-kproto(x=z,k=2)
kpres2

# 3 cluster
kpres3 <-kproto(x=z,k=3)
kpres3

# 4 cluster
kpres4 <-kproto(x=z,k=4)
kpres4

# # Plots
# clprofiles(kpres2, z, col=wes_palette("Royal1",2, type="continuous")) 
# clprofiles(kpres3, z, col=wes_palette("Royal1",2, type="continuous")) 

# Scree plot
n.scree <- ncol(z) - 2
Es <- numeric(n.scree)
for(i in 1:n.scree){ 
  kpres <- kproto(z, k = i, nstart = 5, verbose = FALSE) 
  Es[i] <- kpres$tot.withinss
  } 

plot(1:n.scree, Es[1:4], type = "b", ylab = "Objective Function",
     xlab = "# Clusters", main = "ScreePlot")

# Add with three clusters
group1.9$cluster4 <- kpres4$cluster

```

# 9. You are about to start doing supervised machine learning to predict debt relief. Before you proceed any further, sit with your group and make sure you are content with the measurement of all the variables. 
## a. Check missing values on merged data. Impute/delete as necessary and document accordingly. 
## b. Identify which variables should be in the model (i.e. Should date of complaint be included?) You do not need to keep all of the variables, you can choose to drop the ones you think add less value. You should at least 2 keep variables from each data set. 
## c. Check the measurement of each variable (i.e. Should date be measure, by day or year?) Should something be logged? 
## d. Check all the factors to make sure they have enough observations at each level. 
## e. Optional: You may want to make a design matrix that that includes only the independent variables you want in the model and drops the id variables such as zip code and Complaint ID. 
## f. You may add Clusters and principal components from other variables if you think it will help.
## g. You are absolutely allowed to add additional variables from other public sources to your dataset. This is in no way required. 
```{r}
# Turn some variables into factors
lapply(group1.9, is.factor)
group1.9$Sub.product <- as.factor(group1.9$Sub.product)
group1.9$Sub.issue <- as.factor(group1.9$Sub.issue)
group1.9$Issue <- as.factor(group1.9$Issue)
group1.9$Company.public.response <- as.factor(group1.9$Company.public.response)
group1.9$Submitted.via <- as.factor(group1.9$Submitted.via)
group1.9$Timely.response. <- as.factor(group1.9$Timely.response.)
group1.9$Consumer.disputed. <- as.factor(group1.9$Consumer.disputed.)
group1.9$cluster4 <- as.factor(group1.9$cluster4)

# Check factor levels before removing variables
nlevels(as.factor(group1.9$Company)) # over 2000 levels with very few obs. per level

# Drop medical debt, old_service_mem, Sopoc, SoslhwsldidA, consumer disputed, pct_old out of the data because they are represented in the cluster. Drop state, zip code (keep the imputed one), county, product (all debt collection), customer complaint narrative (unique), complaint ID (unique), date sent (because it is nearly identical to date received), company (over 2000 levels)
group2.0 <- group1.9 %>%
  select(-c(State, ZIP.code, Sub.issue, Company.public.response, Consumer.disputed., pct_young, SoslhwsldidA, old_service_mem, medical_debt, Consumer.complaint.narrative, Complaint.ID, Product, COUNTYNAME, STCOUNTYFP, AGEGRP, Company, Date.sent.to.company))

# Extract year from date received
group2.0$year <- format(as.Date(group2.0$Date.received, format="%m/%d/%y"),"%y")
group2.0$year <- paste0("20", group2.0$year)

# Drop most of the Census variables, date received (because we have the year now)
group2.1 <- group2.0[, -c(1:3, 12:88, 99:100)]

# Recode levels of subproduct
group2.1 <- group2.1 %>%
  mutate(Sub.product = case_when(Sub.product == "Auto" ~ "Auto debt",
                                 Sub.product == "Credit card" ~ "Credit card debt",
                                 Sub.product == "Federal student loan" ~ "Federal student loan debt",
                                 Sub.product == "Mortgage" ~ "Mortgage debt",
                                 Sub.product == "Other (i.e. phone, health club, etc.)" ~ "Other debt",
                                 Sub.product == "Payday loan" ~ "Payday loan debt",
                                 Sub.product == "Medical" ~ "Medical debt",
                                 .default = Sub.product))
table(group2.1$Sub.product)

```

# 10. Using k-fold cross validation, determine the best machine learning model to predict debt relief. At minimum you must run: 
## a.	A linear probability model (can be OLS, ridge, lasso, …) 
## b.	A logistic model
## c.	A CART / regression tree
## d.	Random Forest
## e.	Gradient Boosting/ XGBoost
```{r, results = FALSE}
# LM Stepwise
lm <- lm(relief ~., data = group2.1)
summary(lm)
lm.step <- step(lm, direction = "both") # AIC = -100915.5

# New dataset with the variables stepwise chose less Sub.issue because 
group2.2 <- group2.1 %>%
  select(c(relief, Sub.product, Issue, Tags, Consumer.consent.provided., Submitted.via, Timely.response., pct_hisp, pct_women, AldrA, CcddrA, CcddrCoc, Sopoc, PC1, cluster4, year))


# LOGIT
set.seed(182654)
train <- sample(1:nrow(group2.2), nrow(group2.2)/2)
train.data <- group2.2[train,]
test.data <- group2.2[-train,]

glm.logit <- glm(relief ~ ., data = train.data, family = binomial(link = "logit")) # AIC = 17636, def wrong link function
plot(glm.logit)
summary(glm.logit) 


# LASSO
set.seed(182654)
x <- model.matrix(group2.2$relief ~ ., group2.2)
y <- group2.2$relief
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]
grid <- 10^seq(10, -2, length = 100)

# Run model
ridge.glm <- glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
plot(ridge.glm)

# Run cross validation to find the best lambda
ridge.cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
plot(ridge.cv.out)
bestlam.ridge <- ridge.cv.out$lambda.min
bestlam.ridge # 0.0029565

# Predict using the best lambda
ridge.pred <- predict(ridge.glm, s = bestlam.lasso, newx = x[test,])
mse.ridge <- mean((ridge.pred - y.test)^2) # 0.1153707

# What variables should we take out?
predict(ridge.glm, type = "coef", s = bestlam.ridge)

# CART/Regression Tree
relief.tree <- rpart(relief ~., data = train.data, method = "anova")
printcp(relief.tree)
rpart.plot(relief.tree)
relief.tree$variable.importance

y_pred <- predict(relief.tree, test.data, type = "class")
y_pred <- round(y_pred, 0)
c_matrix <- table(test.data$relief, y_pred)
c_matrix


```

